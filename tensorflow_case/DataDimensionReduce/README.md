数据降维主要方法：
1.缺失值比率 (Missing Values Ratio)
    假设：包含太多缺失值的数据列包含有用信息的可能型较少。
    办法：将数据列缺失值大于某个阀值的列去掉。阀值越高，降维方法越积极，降维后维度越少；
2.低方差滤波（Low Variance Filter）
    假设：数据列变化非常小的列包含的信息量少。
    办法：所有的数据列方差小的列被移除。需要注意的是，方差与数据范围相关，采用该方法前需做归一化处理；
3.高相关滤波（High Correlation Filter）
    假设：当两列数据变化趋势相似时，他们包含的信息也相似。
    办法：这时使用相似列中的一列就可以满足机器学习模型。
        数值列：计算相关性系数；
        标称列：计算皮尔逊卡方值；
        相关系数大于阀值时只保留其中一列。需要注意的是，相关性系数对范围敏感，计算前先归一化处理；
4.随机森林/组合树（Random Forests）
    组合决策树通常被称为随机森林，在进行特征选择与构建有效的分类器时有用。
    办法：对目标属性产生许多巨大的树，然后根据对每个属性的统计结果找到信息量最大的特征子集。
    例：我们能够对一个非常大的数据集生成层次非常浅的树，每棵树只训练一小部分属性。
        如果一个属性经常成为最佳分裂属性，那么它很有可能是需要保留的特征信息。
        对随机森林数据属性的统计评分回向我们揭示与其他属性相比，哪个属性才是预测能力最好的属性。
5.主成分分析（PCA）
    是一个统计过程，通过正交变换将原始的n维数据集变换到一个新的被称为主成分的数据集中。
    变换后的结果中，第一个主成份具有最大的方差之，每个后续的成分在与前述主成分正交条件限制下具有最大方差。
    降维时仅保存前m（m < n）个主成分即可保持最大的数据信息量。
    需要注意的是主成分变换对正交向量的尺度敏感，数据在变换前同样需要进行归一化处理。
    同时，新的主成分也不是由实际系统产生，因此在PCA变换后会丧失数据的解释性。（如果数据的解释能力对你很重要，那么PCA不适用）
6.反向特征消除（Backward Feature Elimination）
    先用n个特征进行训练。每次降维操作，采用n-1个特征对分类器训练n次，得到新的n个分类器。
        将新分类器中错分率变化最小的分类器所用的n-1维特征作为降维后的特征集。
    不断对该过程进行迭代，即可得到降维后的结果。
    第k次迭代过程中得到的是n-k维特征分类器，通过选择最大的错误容忍率，可以得到在选择分类器上达到指定分类性能最少需要多少个特征。
7.前向特征构造（Forward Feature Construction）
    是反向特征消除的反过程。
    从第1个特征开始，每次训练添加一个让分类器性能提升最大的特征。
    前向和反向都非常耗时，通常用于输入维度已经相对较低的数据集。

